{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1129259b",
   "metadata": {},
   "source": [
    "## Simple RAG implementation using LangChain and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67524702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anupeshkumar.verma/Downloads/Projects/AI_ML/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff9f47",
   "metadata": {},
   "source": [
    "#### Make global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f0248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF_URL = \"https://www.upl-ltd.com/images/people/downloads/Leave-Policy-India.pdf\"\n",
    "PDF_URL = \"/Users/anupeshkumar.verma/Downloads/Personal/Anupesh_Resume.pdf\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"                 # \"all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_ID = \"google/flan-t5-base\"                                            # \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4311e",
   "metadata": {},
   "source": [
    "### Load and split PDF into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndSplitPDF():\n",
    "    documents = PyPDFLoader(PDF_URL).load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c71ef",
   "metadata": {},
   "source": [
    "### Create vector embeddings and store in FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb73c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createStoreVectorEmbeddings():\n",
    "    docs = loadAndSplitPDF()\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = EMBEDDING_MODEL_NAME)\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3d5c6",
   "metadata": {},
   "source": [
    "### Select model and create pipeline and huggingface wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d77d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLLM():\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=LLM_MODEL_ID,\n",
    "        tokenizer=LLM_MODEL_ID,\n",
    "        max_new_tokens=256,     # Reduced max_new_tokens for conciseness\n",
    "        # temperature=0.1,       # Lower temperature for less randomness and more factual answers\n",
    "        do_sample=False,\n",
    "        device = -1             # set to -1 if no GPU available\n",
    "    )\n",
    "    rag_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    return rag_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311a932",
   "metadata": {},
   "source": [
    "### Create a RetrievalQA chain combining the vector store retriever and the LLM\n",
    "- k indicates the number of top documents to retrieve that are most similar to the query. In this case, k=2 means the retriever will return the 2 most relevant documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f0b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetrievalQAChain():\n",
    "    vector_store = createStoreVectorEmbeddings()\n",
    "    rag_llm = loadLLM()\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=rag_llm,\n",
    "        chain_type=\"stuff\",  # simple concatenation of retrieved docs\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c98071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(query):\n",
    "    rag_chain = createRetrievalQAChain()\n",
    "    response = rag_chain.invoke({\"query\": query})\n",
    "    print(f\"Question: {query}\\nAnswer: {response[\"result\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef47a3b",
   "metadata": {},
   "source": [
    "### Ask queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2911cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Hi\n",
      "Answer: Hi Anupesh Kumar Verma Anupesh Kumar Verma Anupesh Kumar Verma121@gmail.com Anupesh Kumar Verma121@gmail.com /ne+91 9794371985 7Portfolio | /nednLinkedIn | /gtbGitHub Education • Motilal Nehru National Institute of Technology Allahabad (NIT Allahabad) 2024 Bachelor of Technology (B.Tech) CGPA: 7.9/10 Electronics and Communication Engineering Experience • Data Engineer Trainee | Personify Health Jan 2025 - Present  Collaboration: Working closely with data analysts and other stakeholders to understand data requirements and deliver actionable insights.\n"
     ]
    }
   ],
   "source": [
    "# Query RAG\n",
    "query = \"What types of leaves are covered in this policy?\"\n",
    "query = \"What is the current role of Anupesh ?\"\n",
    "generateResponse(\"Hi\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
